{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYCT8ulkaqOQ"
      },
      "source": [
        "## **Определение линейной регрессии**\n",
        "Линейная регрессия (Linear regression) — один из простейший алгоритмов машинного обучения, описывающий зависимость целевой переменной от признака в виде линейной функции $y = kx + b$. В данном случае была представлена простая или парная линейная регрессия, а уравнение вида $f_{w, b}(x) = w_0x_0 + w_1x_1 +... + w_{n}x_{n} + b = w \\cdot x + b$ называется множественной линейной регрессией, где $b$ — смещение модели, $w$ — вектор её весов, а $x$ — вектор признаков одного обучающего образца.\n",
        "\n",
        "К другим условиям определения линейной регрессии относятся гомоскедастичность (дисперсия остатков постоянна и конечна), а также отсутствие мультиколлинеарности (линейной зависимости между признаками).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN2UuRiWdhzn"
      },
      "source": [
        "![Linear regression scheme.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQtGaY_BorOW"
      },
      "source": [
        "### **Метод наименьших квадратов и функция потерь**\n",
        "Выбор регрессионной линии (плоскости), описывающей взаимосвязь данных наилучшим образом, заключается в минимизации функции потерь $J$, представленной в виде среднеквадратичной ошибки. Проще говоря, линия должна проходить через данные таким образом, чтобы в среднем разница квадратов ожидаемых и реальных значений была минимальна. Данный метод называется методом наименьших квадратов.\n",
        "\n",
        "При наличии же большого количества выбросов в данных, более эффективным может оказаться метод наименьших модулей, однако у него есть один серьёзный недостаток: функция модуля не является дифференцируемой в точке $x = 0$, что в ряде случаев может затруднить минимизацию ошибки модели.\n",
        "Следовательно, исходя из теоремы [Гаусса-Маркова](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%93%D0%B0%D1%83%D1%81%D1%81%D0%B0_%E2%80%94_%D0%9C%D0%B0%D1%80%D0%BA%D0%BE%D0%B2%D0%B0), метод наименьших квадратов является наиболее оптимальной оценкой параметров модели среди всех линейных и несмещённых оценок за счёт меньшей дисперсии.\n",
        "\n",
        "**Вывод нормального уравнения (least squares normal equation)**\n",
        "\n",
        "$J(w, X, y) =\n",
        "\\frac{1}{2n} \\sum\\limits_{i = 1}^{n} (w^T x_i - y_i)^2 =\n",
        "\\frac{1}{2n} ||X w - y||^{2} =\n",
        "\\frac{1}{2n} (X w - y)^T (X w - y) = \\\\\n",
        "= \\frac{1}{2n} ((X w)^T X w - (X w)^T y - y^T X w + y^T y) = |(X w)^T y = y^T X w| = \\\\\n",
        "= \\frac{1}{2n} (w^T X^T X w - 2 y^T X w + y^T y)$\n",
        "\n",
        "$\\frac{\\partial J}{\\partial w} = 0 \\ \\ \\Rightarrow \\ \\ \\frac{1}{2n} (2X^TX w - 2X^T y) = 0 \\ \\ \\Rightarrow \\ \\ X^TX w = X^T y\n",
        "\\ \\ \\Rightarrow \\ \\ w = (X^TX)^{-1} X^T y$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFSGp8wIoz-F"
      },
      "source": [
        "### **Принцип работы линейной регрессии**\n",
        "Существуют 2 основных способа обучения линейной регрессии:\n",
        "- 1) Прямое (нормальное) уравнение в аналитической виде $w = (X^{T}X)^{-1}X^{T} y$, где в данном случае $w$ — вектор весов, включая смещение $b$. Главный недостаток данного способа заключается в высокой вычислительной сложности при большом количестве признаков.\n",
        "- 2) Итеративная оптимизация с постепенным снижением ошибки модели на основе градиентного спуска и его разновидностей. Именно данный способ чаще всего используется на практике.\n",
        "\n",
        "**Линейная регрессия на основе градиентного спуска строится следующим образом**:\n",
        "- 1) изначально устанавливаются нулевые значения для весов, смещения и их градиентов;\n",
        "- 2) на основе установленных значений делается прогноз;\n",
        "- 3) на основе полученного прогноза пересчитываются значения весов и смещения, а также снижение их градиентов (разность значений на текущей и предыдущей итерациях);\n",
        "- 4) шаги 2-3 повторяются до тех пор, пока снижение градиентов не станет меньше заранее установленного порогового значения;\n",
        "- 5) итоговым прогнозом будет линейная комбинация полученных весов + смещение и признаков на тестовой выборке."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8i59oLvo6OK"
      },
      "source": [
        "**Формулы для расчётов**\n",
        "\n",
        "$\\begin{align}\n",
        "\\frac{\\partial J(\\Theta)}{\\partial b}  &= \\frac{1}{n} \\sum\\limits_{i = 1}^{n} (f_{w ,b}(x_i) - y_i) \\\\\n",
        "\\frac{\\partial J(\\Theta)}{\\partial w}  &= \\frac{1}{n} \\sum\\limits_{i = 1}^{n} (f_{w ,b}(x_i) - y_i) x_i \\\\\n",
        "\\end{align}$\n",
        "\n",
        "$\\begin{align} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\\;\n",
        "& b_j = b_{j-1} - \\alpha \\frac{\\partial J(\\Theta)}{\\partial b_j} \\\\\n",
        "& w_j = w_{j-1} - \\alpha \\frac{\\partial J(\\Theta)}{\\partial w_j}\n",
        "\\newline \\rbrace\n",
        "\\end{align}$\n",
        "\n",
        "$\\alpha$ — learning rate\n",
        "\n",
        "$j$ — current iteration for bias and weights\n",
        "\n",
        "$\\Theta$ — parameters matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZB5yeECawSU"
      },
      "source": [
        "### **Реализация на Python с нуля**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bcK0YXBcZpid"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import scale, PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uE-5bxMCSQHi"
      },
      "outputs": [],
      "source": [
        "class MatrixLinearRegression:\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.insert(X, 0, 1, axis=1)   # add ones vector\n",
        "        XT_X_inv = np.linalg.inv(X.T @ X)   # (X.T * X) ** (-1) inverse matrix\n",
        "        weights = np.linalg.multi_dot([XT_X_inv, X.T, y])   # XT_X_inv * X.T * y\n",
        "        self.bias, self.weights = weights[0], weights[1:]\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        return X_test @ self.weights + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "y06fN21Ha6Fo"
      },
      "outputs": [],
      "source": [
        "class GDLinearRegression:\n",
        "    def __init__(self, learning_rate=0.01, tolerance=1e-8):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.tolerance = tolerance\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.bias, self.weights = 0, np.zeros(n_features)\n",
        "        previous_db, previous_dw = 0, np.zeros(n_features)\n",
        "\n",
        "        while True:\n",
        "            y_pred = X @ self.weights + self.bias\n",
        "            db = 1 / n_samples * np.sum(y_pred - y)\n",
        "            dw = 1 / n_samples * X.T @ (y_pred - y)\n",
        "            self.bias -= self.learning_rate * db\n",
        "            self.weights -= self.learning_rate * dw\n",
        "\n",
        "            abs_db_reduction = np.abs(db - previous_db)\n",
        "            abs_dw_reduction = np.abs(dw - previous_dw)\n",
        "\n",
        "            if abs_db_reduction < self.tolerance:\n",
        "                if abs_dw_reduction.all() < self.tolerance:\n",
        "                    break\n",
        "\n",
        "            previous_db = db\n",
        "            previous_dw = dw\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        return X_test @ self.weights + self.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzUAZ7VlalE3"
      },
      "source": [
        "### **Загрузка датасета**\n",
        "Для обучения моделей будет использован [Multiple Linear Regression Dataset](https://www.kaggle.com/datasets/hussainnasirkhan/multiple-linear-regression-dataset), где необходимо спрогнозировать доход сотрудников на основе их возраста и опыта."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "sXF-sD6gQgM1",
        "outputId": "68b7f201-e90d-40ab-a037-3bcda5fa504f"
      },
      "outputs": [],
      "source": [
        "df_path = \"/content/drive/MyDrive/income.csv\"\n",
        "income = pd.read_csv(df_path)\n",
        "X1, y1 = income.iloc[:, :-1].values, income.iloc[:, -1].values\n",
        "X1_scaled = scale(X1)\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, random_state=0)\n",
        "X1_train_s, X1_test_s, y1_train, y1_test = train_test_split(X1_scaled, y1, random_state=0)\n",
        "print(income)\n",
        "\n",
        "correlation_matrix = income.corr()\n",
        "correlation_matrix.style.background_gradient(cmap='coolwarm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKwKBIJd4nhX"
      },
      "source": [
        "### **Обучение моделей и оценка полученных результатов**\n",
        "Как и ожидалось, линейная регрессия показала хорошие результаты в связи с линейной зависимостью в используемых данных. На графике также видно как полученная плоскость хорошо описывает линейную взаимосвязь в данных. Стоит также добавить, что в многомерном пространстве вместо линии или плоскости связь в данных будет описываться гиперплоскостью размерностью $n-1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQWw6kVB6EdN"
      },
      "source": [
        "**Linear regression (matrix method)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJ3I8KPnTkfX",
        "outputId": "594ce0cd-f9e2-45f7-82c3-d560ef282636"
      },
      "outputs": [],
      "source": [
        "matrix_linear_regression = MatrixLinearRegression()\n",
        "matrix_linear_regression.fit(X1_train_s, y1_train)\n",
        "matrix_lr_pred_res = matrix_linear_regression.predict(X1_test_s)\n",
        "matrix_lr_r2 = r2_score(y1_test, matrix_lr_pred_res)\n",
        "matrix_lr_mape = mean_absolute_percentage_error(y1_test, matrix_lr_pred_res)\n",
        "\n",
        "print(f'Matrix Linear regression  R2 score: {matrix_lr_r2}')\n",
        "print(f'Matrix Linear regression MAPE: {matrix_lr_mape}', '\\n')\n",
        "\n",
        "print(f'weights: {matrix_linear_regression.bias, *matrix_linear_regression.weights}')\n",
        "print(f'prediction: {matrix_lr_pred_res}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDHKlncM3Y-6"
      },
      "source": [
        "**Linear regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epLtfD1J9XtD",
        "outputId": "ac1b9f35-bbda-4ba6-f397-ff417d6299fa"
      },
      "outputs": [],
      "source": [
        "linear_regression = GDLinearRegression()\n",
        "linear_regression.fit(X1_train_s, y1_train)\n",
        "pred_res = linear_regression.predict(X1_test_s)\n",
        "r2 = r2_score(y1_test, pred_res)\n",
        "mape = mean_absolute_percentage_error(y1_test, pred_res)\n",
        "\n",
        "print(f'Linear regression R2 score: {r2}')\n",
        "print(f'Linear regression MAPE: {mape}', '\\n')\n",
        "\n",
        "print(f'weights: {linear_regression.bias, *linear_regression.weights}')\n",
        "print(f'prediction: {pred_res}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whXhaOHd3Szc"
      },
      "source": [
        "**Linear regression (scikit-learn)**\n",
        "\n",
        "Как можно было заметить, в данном случае получились другие веса так как в предыдущих случаях модели обучались на масштабированных признаках. Если текущую модель обучить на масштабированных признаках, то веса будут такие же как и в случаях выше."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOYZpIFOqq60",
        "outputId": "65217b1d-a636-4e33-9b45-286860edee7d"
      },
      "outputs": [],
      "source": [
        "sk_linear_regression = LinearRegression()\n",
        "sk_linear_regression.fit(X1_train, y1_train)\n",
        "\n",
        "sk_lr_pred_res = sk_linear_regression.predict(X1_test)\n",
        "sk_lr_r2 = r2_score(y1_test, sk_lr_pred_res)\n",
        "sk_lr_mape = mean_absolute_percentage_error(y1_test, sk_lr_pred_res)\n",
        "\n",
        "print(f'Scikit-learn Linear regression R2 score: {sk_lr_r2}')\n",
        "print(f'Scikit-learn Linear regression MAPE: {sk_lr_mape}', '\\n')\n",
        "\n",
        "print(f'weights: {sk_linear_regression.intercept_, *sk_linear_regression.coef_}')\n",
        "print(f'prediction: {sk_lr_pred_res}', '\\n')\n",
        "\n",
        "sk_linear_regression.fit(X1_train_s, y1_train)\n",
        "print(f'scaled weights: {sk_linear_regression.intercept_, *sk_linear_regression.coef_}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjTJEi6DuPz6"
      },
      "source": [
        "**Визуализация множественной линейной регрессии с 2 признаками**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "yHlbktBfb622",
        "outputId": "bed4e633-8928-40ad-dcd4-27ff34255a43"
      },
      "outputs": [],
      "source": [
        "feature1, feature2 = X1[:, 0], X1[:, 1]\n",
        "X1_linspace = np.linspace(feature1.min(), feature1.max())\n",
        "X2_linspace = np.linspace(feature2.min(), feature2.max())\n",
        "X1_surface, X2_surface = np.meshgrid(X1_linspace, X2_linspace)\n",
        "X_surfaces = np.array([X1_surface.ravel(), X2_surface.ravel()]).T\n",
        "\n",
        "sk_linear_regression = LinearRegression()\n",
        "sk_linear_regression.fit(X1_train, y1_train)\n",
        "y_surface = sk_linear_regression.predict(X_surfaces).reshape(X1_surface.shape)\n",
        "\n",
        "fig = plt.figure(figsize=(9, 7))\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.scatter(feature1, feature2, y1, color='red', marker='o')\n",
        "ax.plot_surface(X1_surface, X2_surface, y_surface, color='black', alpha=0.6)\n",
        "plt.title('Fitted linear regression surface')\n",
        "ax.set_xlabel('Age')\n",
        "ax.set_ylabel('Experience')\n",
        "ax.set_zlabel('Income')\n",
        "ax.view_init(20, 10)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAkREukQwaHT"
      },
      "source": [
        "## **Полиномиальная регрессия**\n",
        "Линейную регрессию также можно применять к данным с нелинейной зависимостью, добавив степени каждого признака в виде новых признаков с последующим обучением на полученном датасете. Такой подход позволяет улавливать линейные связи в многомерном пространстве признаков и называется **полиномиальной регрессией**, а для преобразования признаков в полином степени n в scikit-learn имеется класс PolynomialFeatures, который кроме степеней каждого признака ещё добавляет их комбинации до заданной степени. Например, для признаков $a$ и $b$ со степенью 3 кроме $a^2$, $a^3$ и $b^2$, $b^3$ будут также добавлены их комбинации в виде $ab$, $a^2b$ и $ab^2$.\n",
        "\n",
        "Стоит отметить, что полиномиальная регрессия всё ещё остаётся линейной, однако на графике в исходном пространстве признаков она будет выглядеть в виде кривой, поскольку полученная гиперплоскость в многомерном пространстве признаков будет соответствовать сложной кривой линии в исходном пространстве."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgMViwQgwh1D"
      },
      "source": [
        "### **Загрузка датасета**\n",
        "Для обучения модели будет использован [Ice Cream Selling dataset](https://www.kaggle.com/datasets/mirajdeepbhandari/polynomial-regression) где необходимо выполнить прогнозирование продаж мороженого на основе данных о температуре."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nRAhhpyCSy7",
        "outputId": "5fa85c5d-14ad-4faa-866e-41f62da776b6"
      },
      "outputs": [],
      "source": [
        "df_path = \"/content/drive/MyDrive/ice_cream_selling.csv\"\n",
        "ice_cream = pd.read_csv(df_path)\n",
        "X2, y2 = ice_cream.iloc[:, :-1], ice_cream.iloc[:, -1]\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, random_state=0)\n",
        "print(ice_cream.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxjXBojdwe3D"
      },
      "source": [
        "### **Обучение полиномиальной регрессии и оценка полученных результатов**\n",
        "Как можно заметить, в данном случае линейная регрессия не способна описать взаимосвязь в исходных данных должным образом и имеет высокую ошибку, однако после добавления полиномиальных признаков результат получился довольно хорошим.\n",
        "\n",
        "С другой стороны, если использовать полином слишком высокой степени, то модель будет явно переобучена, а если низкой, то недообучена. Для решения данной проблемы необходимо использовать перебор нескольких полиномов для выбора оптимальной кривизны линии, описывающей взаимосвязь в данных наилучшим образом."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "xwrJGRbcsOUr",
        "outputId": "bc8ec7e9-138e-4165-f658-e5ff0d133f94"
      },
      "outputs": [],
      "source": [
        "feature_name, target_name = ice_cream.columns\n",
        "poly_features = PolynomialFeatures(degree=8, include_bias=True)\n",
        "X2_poly = poly_features.fit_transform(X2)\n",
        "X2_poly_train, X2_poly_test = train_test_split(X2_poly, random_state=0)\n",
        "\n",
        "sk_linear_regression = LinearRegression()\n",
        "sk_linear_regression.fit(X2_train, y2_train)\n",
        "sk_lr_pred_res = sk_linear_regression.predict(X2_test)\n",
        "sk_lr_pred_all_data_res = sk_linear_regression.predict(X2)\n",
        "\n",
        "sk_polynomial_regression = LinearRegression()\n",
        "sk_polynomial_regression.fit(X2_poly_train, y2_train)\n",
        "sk_poly_lr_pred_res = sk_polynomial_regression.predict(X2_poly_test)\n",
        "sk_poly_lr_pred_all_data_res = sk_polynomial_regression.predict(X2_poly)\n",
        "\n",
        "linear_regression_r2 = r2_score(y2_test, sk_lr_pred_res)\n",
        "polynomial_regression_r2 = r2_score(y2_test, sk_poly_lr_pred_res)\n",
        "\n",
        "linear_regression_mse = mean_squared_error(y2_test, sk_lr_pred_res)\n",
        "polynomial_regression_mse = mean_squared_error(y2_test, sk_poly_lr_pred_res)\n",
        "\n",
        "linear_regression_mape = mean_absolute_percentage_error(y2_test, sk_lr_pred_res)\n",
        "polynomial_regression_mape = mean_absolute_percentage_error(y2_test, sk_poly_lr_pred_res)\n",
        "\n",
        "print(f'R2 score (Linear regression): {linear_regression_r2}')\n",
        "print(f'R2 score (Polynomial regression): {polynomial_regression_r2}', '\\n')\n",
        "\n",
        "print(f'MSE (Linear regression): {linear_regression_mse}')\n",
        "print(f'MSE (Polynomial regression): {polynomial_regression_mse}', '\\n')\n",
        "\n",
        "print(f'MAPE (Linear regression): {linear_regression_mape}')\n",
        "print(f'MAPE (Polynomial regression): {polynomial_regression_mape}')\n",
        "\n",
        "plt.scatter(X2, y2)\n",
        "plt.plot(X2, sk_lr_pred_all_data_res, color='darkorange', label='Linear Regression')\n",
        "plt.plot(X2, sk_poly_lr_pred_all_data_res, color='green', label='Polynomial Regression')\n",
        "plt.title('Polynomial vs Linear regression')\n",
        "plt.xlabel(feature_name)\n",
        "plt.ylabel(target_name)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p08Fr4G4NSHu"
      },
      "source": [
        "## **Регуляризация линейной регрессии (Ridge, Lasso, ElasticNet)**\n",
        "Если в полиномиальной регрессии регуляризация позволяет уменьшить кривизну линии через снижение количества полиномиальных степеней, то в случае с линейной регрессией регуляризация будет заключаться в изменении наклона линии путём ограничения весов модели, обменивая более высокое смещение на низкую дисперсию.\n",
        "\n",
        "**Гребневая регрессия (Ridge regression)** или *регуляризация Тихонова* применяется в случае мультиколлинеарности через добавление L2-регуляризации к функции потерь во время обучения и сильнее всего занижает веса для признаков с высокой корреляцией: их значения будут приближаться к нулю, но никогда его не достигнут. Лучше всего применять гребневую регрессию после стандартизации признаков.\n",
        "\n",
        "$J += \\alpha \\sum \\limits_{i=1}^{n} w_i^2$\n",
        "\n",
        "**Лассо-регрессия (Lasso regression или Least Absolute Shrinkage & Selection Operator)** обычно используется для отбора признаков через добавление L1-регуляризации к функции потерь во время обучения. Проще говоря, лассо-регрессия стремится уменьшить число параметров модели путем зануления весов для неинформативных и избыточных признаков, что на выходе даст разреженную модель (с небольшим числом ненулевых весов признаков).\n",
        "\n",
        "$J += \\alpha \\sum \\limits_{i=1}^{n} |w_i|$\n",
        "\n",
        "**Эластичная сеть (ElasticNet)** представляет собой комбинацию L1 и L2-регуляризаций через отношение их смеси $r$, что может принести особую пользу в ситуациях, когда в данных необходимо одновременно выполнять отбор признаков и бороться с мультиколлинеарностью. В Scikit-Learn для управления смесью Ridge и Lasso используется \"l1_ratio\".\n",
        "\n",
        "$J = MSE(w) + r\\alpha \\sum \\limits_{i=1}^{n} |w_i| +\n",
        "\\frac{1 - r}{2} \\alpha \\sum \\limits_{i=1}^{n} w_i^2$\n",
        "\n",
        "Кроме того, в качестве регуляризации линейной регрессии ещё можно использовать **раннюю остановку (early stopping)**, которая заключаются в прекращении обучения модели после определённого количества итераций или достижения заданного уровня ошибки на валидационной выборке. Однако при использовании стохастического или мини-пакетного градиентного спуска в данном случае могут вознинкуть трудности в поиске минимальной ошибки из-за менее гладких кривых обучения.\n",
        "\n",
        "Стоит также отметить, что приведённые выше методы регуляризации нередко показывают хороший прирост в точности на зашумленных данных, а в случае с полиномиальными признаками можно достичь чуть меньшего искривления линии, что также позволит увеличить обобщающую способность модели.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ9VxmQtz1NU"
      },
      "source": [
        "### **Загрузка датасета**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0i0dMqf78Sl",
        "outputId": "54884d7b-cf45-403b-8f0a-b23f0e7bd68b"
      },
      "outputs": [],
      "source": [
        "X3, y3 = make_regression(n_samples=14, n_features=1, noise=2, random_state=0)\n",
        "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, random_state=0)\n",
        "print('X3', X3, sep='\\n')\n",
        "print('y3', y3, sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLL0rc3wAJ2a"
      },
      "source": [
        "### **Обучение моделей и оценка полученных результатов**\n",
        "Как можно заметить, в данном случае ElasticNet имеет самую высокую точность среди всех видов регуляризации, что обусловлено лучшим увеличением обобщающей способности за счёт более сильных штрафов во время обучения. На графике ниже видно, что это достигается за счёт более сильного наклона красной линии."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwUphg6zJOhc"
      },
      "source": [
        "**Linear regression (scikit-learn)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvws4sDOJePy",
        "outputId": "a722ac78-f492-47eb-d9c3-ded566e03b4a"
      },
      "outputs": [],
      "source": [
        "sk_linear_regression = LinearRegression()\n",
        "sk_linear_regression.fit(X3_train, y3_train)\n",
        "\n",
        "sk_lr_pred_res = sk_linear_regression.predict(X3_test)\n",
        "sk_lr_pred_train_res = sk_linear_regression.predict(X3_train)\n",
        "\n",
        "sk_lr_r2 = r2_score(y3_test, sk_lr_pred_res)\n",
        "sk_lr_train_r2 = r2_score(y3_train, sk_lr_pred_train_res)\n",
        "\n",
        "sk_lr_mse = mean_squared_error(y3_test, sk_lr_pred_res)\n",
        "sk_lr_train_mse = mean_squared_error(y3_train, sk_lr_pred_train_res)\n",
        "\n",
        "sk_lr_mape = mean_absolute_percentage_error(y3_test, sk_lr_pred_res)\n",
        "sk_lr_train_mape = mean_absolute_percentage_error(y3_train, sk_lr_pred_train_res)\n",
        "\n",
        "print(f'Linear regression R2 score: {sk_lr_r2}')\n",
        "print(f'Linear regression train R2 score: {sk_lr_train_r2}', '\\n')\n",
        "\n",
        "print(f'Linear regression MSE: {sk_lr_mse}')\n",
        "print(f'Linear regression train MSE: {sk_lr_train_mse}', '\\n')\n",
        "\n",
        "print(f'Linear regression MAPE: {sk_lr_mape}')\n",
        "print(f'Linear regression train MAPE: {sk_lr_train_mape}', '\\n')\n",
        "\n",
        "print(f'prediction: {sk_lr_pred_res}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6YGzf9l215_"
      },
      "source": [
        "**Ridge regression (scikit-learn)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rw9lo-juzw1x",
        "outputId": "ffd702d6-5b4b-4f76-87ec-70bf705b53f1"
      },
      "outputs": [],
      "source": [
        "sk_ridge_regression = Ridge()\n",
        "sk_ridge_regression.fit(X3_train, y3_train)\n",
        "\n",
        "sk_ridge_pred_res = sk_ridge_regression.predict(X3_test)\n",
        "sk_ridge_pred_train_res = sk_ridge_regression.predict(X3_train)\n",
        "\n",
        "sk_ridge_r2 = r2_score(y3_test, sk_ridge_pred_res)\n",
        "sk_ridge_train_r2 = r2_score(y3_train, sk_ridge_pred_train_res)\n",
        "\n",
        "sk_ridge_mse = mean_squared_error(y3_test, sk_ridge_pred_res)\n",
        "sk_ridge_train_mse = mean_squared_error(y3_train, sk_ridge_pred_train_res)\n",
        "\n",
        "sk_ridge_mape = mean_absolute_percentage_error(y3_test, sk_ridge_pred_res)\n",
        "sk_ridge_train_mape = mean_absolute_percentage_error(y3_train, sk_ridge_pred_train_res)\n",
        "\n",
        "print(f'Ridge R2 score: {sk_ridge_r2}')\n",
        "print(f'Ridge train R2 score: {sk_ridge_train_r2}', '\\n')\n",
        "\n",
        "print(f'Ridge MSE: {sk_ridge_mse}')\n",
        "print(f'Ridge train MSE: {sk_ridge_train_mse}', '\\n')\n",
        "\n",
        "print(f'Ridge MAPE: {sk_ridge_mape}')\n",
        "print(f'Ridge train MAPE: {sk_ridge_train_mape}', '\\n')\n",
        "\n",
        "print(f'prediction: {sk_ridge_pred_res}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARhE_FB_2-Dl"
      },
      "source": [
        "**Lasso regression (scikit-learn)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ol3evcMw08V8",
        "outputId": "ed5ee0ca-335e-4ae0-fcce-a135052a5159"
      },
      "outputs": [],
      "source": [
        "sk_lasso_regression = Lasso()\n",
        "sk_lasso_regression.fit(X3_train, y3_train)\n",
        "\n",
        "sk_lasso_pred_res = sk_lasso_regression.predict(X3_test)\n",
        "sk_lasso_pred_train_res = sk_lasso_regression.predict(X3_train)\n",
        "\n",
        "sk_lasso_r2 = r2_score(y3_test, sk_lasso_pred_res)\n",
        "sk_lasso_train_r2 = r2_score(y3_train, sk_lasso_pred_train_res)\n",
        "\n",
        "sk_lasso_mse = mean_squared_error(y3_test, sk_lasso_pred_res)\n",
        "sk_lasso_train_mse = mean_squared_error(y3_train, sk_lasso_pred_train_res)\n",
        "\n",
        "sk_lasso_mape = mean_absolute_percentage_error(y3_test, sk_lasso_pred_res)\n",
        "sk_lasso_train_mape = mean_absolute_percentage_error(y3_train, sk_lasso_pred_train_res)\n",
        "\n",
        "print(f'Lasso R2 score: {sk_lasso_r2}')\n",
        "print(f'Lasso train R2 score: {sk_lasso_train_r2}', '\\n')\n",
        "\n",
        "print(f'Lasso MSE: {sk_lasso_mse}')\n",
        "print(f'Lasso train MSE: {sk_lasso_train_mse}', '\\n')\n",
        "\n",
        "print(f'Lasso MAPE: {sk_lasso_mape}')\n",
        "print(f'Lasso train MAPE: {sk_lasso_train_mape}', '\\n')\n",
        "\n",
        "print(f'prediction: {sk_lasso_pred_res}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R97j-97X3DBl"
      },
      "source": [
        "**ElasticNet regression (scikit-learn)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmPXg5T22E8Z",
        "outputId": "1aaf5a4f-cf02-480d-eed0-aface57b99dd"
      },
      "outputs": [],
      "source": [
        "sk_elastic_net_regression = ElasticNet()\n",
        "sk_elastic_net_regression.fit(X3_train, y3_train)\n",
        "\n",
        "sk_elastic_net_pred_res = sk_elastic_net_regression.predict(X3_test)\n",
        "sk_elastic_net_pred_train_res = sk_elastic_net_regression.predict(X3_train)\n",
        "\n",
        "sk_elastic_net_r2 = r2_score(y3_test, sk_elastic_net_pred_res)\n",
        "sk_elastic_net_train_r2 = r2_score(y3_train, sk_elastic_net_pred_train_res)\n",
        "\n",
        "sk_elastic_net_mse = mean_squared_error(y3_test, sk_elastic_net_pred_res)\n",
        "sk_elastic_net_train_mse = mean_squared_error(y3_train, sk_elastic_net_pred_train_res)\n",
        "\n",
        "sk_elastic_net_mape = mean_absolute_percentage_error(y3_test, sk_elastic_net_pred_res)\n",
        "sk_elastic_net_train_mape = mean_absolute_percentage_error(y3_train, sk_elastic_net_pred_train_res)\n",
        "\n",
        "print(f'ElasticNet R2 score: {sk_elastic_net_r2}')\n",
        "print(f'ElasticNet train R2 score: {sk_elastic_net_train_r2}', '\\n')\n",
        "\n",
        "print(f'ElasticNet MSE: {sk_elastic_net_mse}')\n",
        "print(f'ElasticNet train MSE: {sk_elastic_net_train_mse}', '\\n')\n",
        "\n",
        "print(f'ElasticNet MAPE: {sk_elastic_net_mape}')\n",
        "print(f'ElasticNet train MAPE: {sk_elastic_net_train_mape}', '\\n')\n",
        "\n",
        "print(f'prediction: {sk_elastic_net_pred_res}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e40-GGnp3Ep"
      },
      "source": [
        "**Визуализация снижения ошибок линейной регрессии и её регуляризаций**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "6q_qgE2Rx0Vd",
        "outputId": "5eb5ea51-6294-4b34-e595-86e6707d8597"
      },
      "outputs": [],
      "source": [
        "train_r2_scores = [sk_lr_train_r2, sk_ridge_train_r2, sk_lasso_train_r2, sk_elastic_net_train_r2]\n",
        "test_r2_scores = [sk_lr_r2, sk_ridge_r2, sk_lasso_r2, sk_elastic_net_r2]\n",
        "\n",
        "train_mses = [sk_lr_train_mse, sk_ridge_train_mse, sk_lasso_train_mse, sk_elastic_net_train_mse]\n",
        "test_mses = [sk_lr_mse, sk_ridge_mse, sk_lasso_mse, sk_elastic_net_mse]\n",
        "\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(221)\n",
        "ax1.title.set_text('R2 reduction')\n",
        "ax1.set_xlabel('R2 train')\n",
        "ax1.set_ylabel('test')\n",
        "ax1.scatter(train_r2_scores, test_r2_scores, color='red')\n",
        "ax1.plot(train_r2_scores, test_r2_scores)\n",
        "\n",
        "ax2 = fig.add_subplot(222)\n",
        "ax2.title.set_text('MSE reduction')\n",
        "ax2.set_xlabel('MSE train')\n",
        "ax2.scatter(train_mses, test_mses, color='red')\n",
        "ax2.plot(train_mses, test_mses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "0DDulaWOGGTU",
        "outputId": "3974c35b-7b69-4c2b-8df3-b163f8cb840e"
      },
      "outputs": [],
      "source": [
        "sk_linear_regression_pred_all_data_res = sk_linear_regression.predict(X3)\n",
        "sk_ridge_regression_pred_all_data_res = sk_ridge_regression.predict(X3)\n",
        "sk_lasso_regression_pred_all_data_res = sk_lasso_regression.predict(X3)\n",
        "sk_elastic_net_regression_all_data_pred_res = sk_elastic_net_regression.predict(X3)\n",
        "\n",
        "plt.scatter(X3, y3, color='black')\n",
        "plt.plot(X3, sk_linear_regression_pred_all_data_res, label='Linear regression')\n",
        "plt.plot(X3, sk_ridge_regression_pred_all_data_res, label='Ridge')\n",
        "plt.plot(X3, sk_lasso_regression_pred_all_data_res, label='Lasso')\n",
        "plt.plot(X3, sk_elastic_net_regression_all_data_pred_res, label='ElasticNet')\n",
        "plt.title('Regression regularizations comparison')\n",
        "plt.xlabel('X3')\n",
        "plt.ylabel('y3')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs_enmfjuCn0"
      },
      "source": [
        "## **Преимущества и недостатки линейной регрессии**\n",
        "Преимущества:\n",
        "- простота в реализации и интерпретации;\n",
        "- высокая скорость работы;\n",
        "- относительно хорошая точность в случае с линейной зависимостью в данных.\n",
        "\n",
        "Недостатки:\n",
        "- низкая гибкость и адаптивность из-за предположения о линейности данных;\n",
        "- низкая точность в случае с данными сложной формы, что следует из предыдущего пункта;\n",
        "- чувствительность к шуму и выбросам.\n",
        "\n",
        "Стоит отметить, что перечисленные недостатки касаются классического случая и могут быть частично либо полностью устранены с помощью вышеописанных методов. Однако на сегодняшний день существует ряд более стабильных и эффективных алгоритмов, применяемых в классическом машинном обучении."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vz2i4eJqg7b"
      },
      "source": [
        "## **Дополнительные источники**\n",
        "Документация:\n",
        "- [описание линейных моделей](https://scikit-learn.org/stable/modules/linear_model.html#);\n",
        "- [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html);\n",
        "- [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge);\n",
        "- [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso);\n",
        "- [ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet).\n",
        "\n",
        "Видео про линейную и полиномиальную регрессии: [один](https://www.youtube.com/watch?v=HxXxVuG0xSA), [два](https://www.youtube.com/watch?v=7ArmBVF2dCs).\n",
        "\n",
        "Видео про регуляризацию: [один](https://www.youtube.com/watch?v=7WTMcm7QobQ), [два](https://www.youtube.com/watch?v=Q81RR3yKn30), [три](https://www.youtube.com/watch?v=NGf0voTMlcs), [четыре](https://www.youtube.com/watch?v=1dKRdX9bfIo), [пять](https://www.youtube.com/watch?v=7WTMcm7QobQ).\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
